{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_0GKs62U8BI"
      },
      "source": [
        "# Variational Auto Encoder\n",
        "\n",
        "VAE, 変分オートエンコーダ{cite}`VAE`\n",
        "\n",
        "今回はVAEというAEの亜種について，BoWに対する言語モデル（BoWで表現された文書を生成するようなVAE）を例に紹介します．\n",
        "\n",
        ":::{note}\n",
        "本題に入る前に...   \n",
        "AEではencoderの出力をそのまま入力ベクトルの潜在ベクトル（圧縮表現）として利用していましたね．これに対してVAEは，潜在ベクトルの各要素を正規分布からのサンプルの一つだと仮定します．\n",
        "\n",
        "そのためencoderの出力はそのまま使わず，encoderはデータからガウス分布のパラメータ（平均と標準偏差）を作成するために利用されます．画像データなどをVAEで生成する場合はまさにこのままの考え方でOKです．\n",
        "\n",
        "ですがここではもう少し進んで，文書データの生成過程をVAEを使って再現してみます．文書データと言ってもBoW (Bag-of-Words）なので語順などは無視して単語の出現頻度だけを記録した行列です．\n",
        "\n",
        "さて，もしも全世界にある文書を全て集めてこれたのならば：  \n",
        "1. ある文書の潜在ベクトルは __標準正規分布__ からのサンプリングによって生成されていて，\n",
        "2. その潜在ベクトルを適当な関数（ニューラルネット）でそれぞれの文書用の単語頻度分布（全ての要素が0以上で合計が1のベクトル）に変換して，\n",
        "3. この単語頻度分布に従う離散確率分布からのサンプリングによって文書の中の単語がそれぞれ生成される\n",
        "\n",
        "という流れで文書ができていると考えます．\n",
        "\n",
        "私たちが今持っている文書群はこの母集団からサンプリングしてきた一部に過ぎないのです．そのため，実際に扱うデータの潜在ベクトル（の各要素）は標準正規分布に近いけど少し違うガウス分布に従っていることが想像できます．\n",
        "\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rpQ6dERDU8BL",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4g6ANtlwm5x",
        "outputId": "83f5d7be-a64f-4dc3-d44c-b53fb0028e1e",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_hat.shape=(1, 1000)\n",
            "reconstruction_error=36603.369294606615\n"
          ]
        }
      ],
      "source": [
        "m = 128 # minibatch size\n",
        "v = 1000 # number of features. in this case, vocaburary size\n",
        "k = 20 # latent dimension size.\n",
        "\n",
        "x = np.random.random([1,v])\n",
        "h = np.random.random([1, k])\n",
        "W = np.random.random([k, v])\n",
        "b = np.random.random(v)\n",
        "\n",
        "x_hat = h@W +b\n",
        "print(f\"{x_hat.shape=}\")\n",
        "reconstruction_error = ((x - x_hat)**2).sum(axis=1).mean()\n",
        "print(f\"{reconstruction_error=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cEcWVFfZt-d"
      },
      "source": [
        "::::{note}  \n",
        "ちょっと脱線：文書群からのトピック抽出\n",
        "\n",
        "まずは三層のAEについて思い出しながら考えていきましょう．\n",
        "\n",
        "![](https://cdn-ak.f.st-hatena.com/images/fotolife/n/nkdkccmbr/20161006/20161006215630.png)  \n",
        "オートエンコーダのイメージ  \n",
        "出典：[オートエンコーダ-- ニューラルネットワーク・DeepLearningなどの画像素材　プレゼン・ゼミなどに【WTFPL】](https://nkdkccmbr.hateblo.jp/entry/2016/10/06/222245)\n",
        "\n",
        "一つの文書のBoWベクトル$\\mathbf{x}\\in \\mathbb{R}^{v}$が入力されたEncoderは，これを潜在空間上の点として見ることができるベクトル$\\mathbf{h}\\in \\mathbb{R}^{k}$ へ符号化します．\n",
        "Decoderの活性化関数が恒等関数だとすると， $\\hat{\\mathbf{x}}=\\mathbf{h} \\cdot \\mathbf{W} + \\mathbf{b}$を行なっているわけです．\n",
        "\n",
        "```python\n",
        "# In [1]:\n",
        "m = 128 # minibatch size\n",
        "v = 1000 # number of features. in this case, vocaburary size\n",
        "k = 20 # latent dimension size.\n",
        "\n",
        "x = np.random.random([1,v])\n",
        "h = np.random.random([1, k])\n",
        "W = np.random.random([k, v])\n",
        "b = np.random.random(v)\n",
        "\n",
        "x_hat = h@W +b\n",
        "print(f\"{x_hat.shape=}\")\n",
        "reconstruction_error = ((x - x_hat)**2).sum(axis=1).mean()\n",
        "print(f\"{reconstruction_error=}\")\n",
        "\n",
        "# Out [1]:\n",
        "# x_hat.shape=(1, 1000)\n",
        "# reconstruction_error=19712.92650866515\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ミニバッチごとに処理されるとすると，元行列$\\mathbf{X} \\in \\mathbb{R}^{m\\times v}$,再構成行列$\\hat{\\mathbf{X}} \\in \\mathbb{R}^{m\\times v}$, 潜在空間にマップされた行列$\\mathbf{H}\\in \\mathbb{R}^{m\\times k}$, デコーダの重み行列$\\mathbf{W} \\in \\mathbf{R}^{k\\times v}$であり，biasを無視するとこれはほぼ$\\hat{\\mathbf{X}}=\\mathbf{H} \\cdot \\mathbf{W}$と表せます．再構成行列は元行列に近くなるように訓練するので，このオートエンコーダは元の行列を二つの行列$\\mathbf{H}$と$\\mathbf{W}$に行列分解していると見ることができます．\n",
        "\n",
        "ではここで出てきた$k$とはなんでしょうか？オートエンコーダにおけるボトルネックレイヤの次元数のことなのですが，文書データの行列分解の文脈で見るとどういう意味になりそうですか？\n",
        "\n",
        "これをその文書群の中から見つけた __潜在的なトピック(話題)__ であるとする考え方があります．そうするとつまり，\n",
        "- $\\mathbf{H}$ は文書を話題の濃淡で表現した行列\n",
        "- $\\mathbf{W}$ は語彙を話題の濃淡で表現した行列\n",
        "\n",
        "だということもできそうです．\n",
        "\n",
        "例えば$k=3$として，それぞれのトピックが「スポーツ，政治，経済」に関するものであるとしましょう．ある文書の潜在ベクトルは，その文書が三つのトピックそれぞれにどれだけ属しているのかを表す値を持っていると言えそうです．\n",
        "\n",
        "（もちろんencoderの最終層の活性化関数によっては，潜在変数は負の値を持っている場合があります．この場合は理解しづらいのですが...）\n",
        "\n",
        "また，この時の$\\mathbf{W}$は\n",
        "- 1行目がスポーツトピックの持っている単語分布\n",
        "- 2行目が政治トピックの持っている単語分布\n",
        "- 3行目が経済トピックの持っている単語分布\n",
        "\n",
        "だと見ることができそうです．（確率分布のように正規化されていないですが...）  \n",
        "\n",
        "実はそれぞれのトピックの単語分布の中で，大きい値を持つ単語を10個程度ピックアップすれば，そのトピックがどんなトピックなのかが大体わかります．\n",
        "\n",
        "\n",
        "このような方法を使ってトピックを見つけるタスクを __トピック抽出 （Topic Extraction)__ と呼びます．\n",
        "::::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbchb38iU8BL"
      },
      "source": [
        "## データの準備\n",
        "\n",
        "20newsgroupsを利用します．以下のようにbowを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6uajxXdU8BM",
        "outputId": "bb6901e2-0714-4564-fa70-63a47ab1fa69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "done in 24.573s.\n",
            "Extracting tf features for LDA...\n",
            "done in 3.737s.\n"
          ]
        }
      ],
      "source": [
        "# The number of words in the vocabulary\n",
        "n_words = 1000\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"), )\n",
        "data_samples = dataset.data\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.7, min_df=10, max_features=n_words, stop_words=\"english\", strip_accents=\"ascii\", )\n",
        "\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atsB0TLDWaCC",
        "outputId": "cbfdc73a-5512-4d3a-c933-c295806ecd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sure 873\n",
            "story 861\n",
            "did 301\n",
            "statement 856\n",
            "media 590\n",
            "pro 715\n",
            "israeli 497\n",
            "world 985\n",
            "having 442\n",
            "letter 535\n"
          ]
        }
      ],
      "source": [
        "word2id = tf_vectorizer.vocabulary_\n",
        "# print(\"Number of features = {}\".format(len(word2id)))\n",
        "# out[1]\n",
        "# Number of features = 1000\n",
        "id2word = list(word2id.keys())\n",
        "# print(\"First 20 features = {}\".format(id2word[:20]))\n",
        "# Out[2]\n",
        "# First 20 features = ['sure', 'story', 'did', 'statement', 'media', 'pro', 'israeli', 'world', 'having', 'letter', 'try', 'think', 'reason', 'report', 'clearly', 'reports', 'received', 'government', 'makes', 'away']\n",
        "\n",
        "for count, word in enumerate(word2id):\n",
        "    if count < 10:\n",
        "        print(word, word2id[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-huXZZCU8BN",
        "outputId": "880918a0-252f-441d-fa23-fa0988e51f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of docs for training = 10000\n",
            "Number of docs for test = 1314\n",
            "Number of tokens in training set = 479936\n",
            "Sparsity = 0.0254311\n"
          ]
        }
      ],
      "source": [
        "n_samples_tr = 10000\n",
        "n_samples_te = tf.shape[0] - n_samples_tr\n",
        "docs_tr = tf[:n_samples_tr, :]\n",
        "docs_te = tf[n_samples_tr:, :]\n",
        "print(\"Number of docs for training = {}\".format(docs_tr.shape[0]))\n",
        "print(\"Number of docs for test = {}\".format(docs_te.shape[0]))\n",
        "\n",
        "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
        "print(f\"Number of tokens in training set = {n_tokens}\")\n",
        "print(\n",
        "    \"Sparsity = {}\".format(len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1]))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwJ8ODNIU8BN",
        "outputId": "dd5433db-5bb8-4cb1-f6b0-01c220797cc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 254311 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsUvFsZk4kAU"
      },
      "source": [
        "ちなみに10000x1000の要素を持つ普通の配列が(dtypeがfloat32の場合）どのくらいのメモリを食うのかを確認してみると："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuIuvzpZU8BN",
        "outputId": "c5382eb3-b79f-4b83-a39a-eefc4c464b1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_te.toarray() # こうすれば元の配列に戻せる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ajpN5qxo7_7T"
      },
      "outputs": [],
      "source": [
        "K = 1024\n",
        "byte = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oetUa4boBkTN",
        "outputId": "b0dab370-3208-4332-9666-60c113716ab6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "76.2939453125"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(10000*1000*64)/(K*K*8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntLrHxb5DWXo",
        "outputId": "3740b1b9-1232-4c3c-dd7e-da42797837b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.00762939453125"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b.nbytes / (K*K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8hqcp685SJr"
      },
      "source": [
        "そして疎行列型のdocs_trがどのくらいメモリを食っているのか:\n",
        "\n",
        "```\n",
        "Type:        csr_matrix\n",
        "Docstring:  \n",
        "Compressed Sparse Row matrix\n",
        "...\n",
        "\n",
        "Attributes\n",
        "----------\n",
        "...\n",
        "data\n",
        "    CSR format data array of the matrix\n",
        "indices\n",
        "    CSR format index array of the matrix\n",
        "indptr\n",
        "    CSR format index pointer array of the matrix\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2FK793y5XBP",
        "outputId": "69c4f66c-fe8e-4d24-9a99-9287386ab6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025.87109375 KB\n"
          ]
        }
      ],
      "source": [
        "a = (\n",
        "    docs_tr.indices.nbytes +\n",
        "    docs_tr.indptr.nbytes +\n",
        "    docs_tr.data.astype(np.float32).nbytes\n",
        "    ) / K\n",
        "print(a, \"KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqHGbBDqA2UE",
        "outputId": "e88642a3-0de1-4f6c-9fce-dce2f763da6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1986.8046875"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_tr.nnz*64 / (K*byte)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P515K1UU8BN"
      },
      "source": [
        "## Auto Encoderからstep-by-stepでVAEを構築"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muw_Hx09U8BO"
      },
      "source": [
        "VAEの気持ち：  \n",
        "潜在ベクトルの各要素が標準正規分布に従って生成されていると仮定したAuto Encoder．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuBcOjXF28Q"
      },
      "source": [
        "### 単純なAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3rGaKT2aBTd"
      },
      "source": [
        "入力される特徴量が1000，　隠れ層の次元数が20のオートエンコーダクラスを書いてください．\n",
        "- Encoderの最終層の活性化関数はなんでも良い\n",
        "- Decoderの最終層はLinearとする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ujgSJhJ7mQLX"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_features:int=1000, n_components:int=20):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_components = n_components\n",
        "        # build layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.n_components*2),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.n_components, self.in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        encoded = self.encoder(x)\n",
        "        return encoded, self.decoder(encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZG-sf_rozR"
      },
      "source": [
        "BoWを単純に圧縮した後に再構成するならば，上のようなAEでOKです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Admdjcx4mQm-"
      },
      "source": [
        "ではここで出力される$\\hat{x}$に着目して，その要素一つ一つがこのコーパスに登場した語彙（ユニークな単語）に対応していることを思い出してください．\n",
        "\n",
        "例えば登場する語彙が以下の6個だけだったします．\n",
        "1. Happy\n",
        "1. Life\n",
        "1. Home\n",
        "1. TamaHome\n",
        "1. Unhappy\n",
        "1. House\n",
        "\n",
        "ある文書でそれぞれの単語が出現するかどうかを1~6の目があるサイコロを何回か転がす試行で決めるとしましょう．その文書が実は\"Happy Life Happy Home TamaHome\"というものだったら， この中に登場した単語（トークン）の数は以下の5つになります．\n",
        "\n",
        "1. Happy\n",
        "2. Life\n",
        "3. Happy\n",
        "4. Home\n",
        "5. TamaHome\n",
        "\n",
        "\n",
        "よって6回サイコロを転がせば（うまく行けば）元の文書と同じ単語リストを獲得できそうです．\n",
        "\n",
        "うまく元の単語リストと同じものを元の文書の単語数と同じ5回だけのサンプリングで出すためには，この文書用のサイコロのそれぞれの面を，単語の出現回数に合わせて出やすさを調節してあげる（広くしたり狭くしたりと歪に歪ませる）必要があります．\n",
        "\n",
        "このような歪なサイコロを一回転がすようなサンプリングをするためには，カテゴリカル分布という確率分布を利用します．また複数回転がす場合は，多項分布と呼ばれる確率分布を利用します．\n",
        "\n",
        "そしてこれらの確率分布の持つサイコロの歪さを表現するためには，全ての面の出やすさを持ったベクトルが必要です．このベクトルの各要素は簡単にいうと面の広さや重さに相当するので，0より大きくて1より小さい値になります．そして全ての要素の合計が1になるように調節（正則化）しておく必要があります．\n",
        "\n",
        "このような条件のベクトル作成するのにちょうどいい関数として，私たちはSoftmax関数を知っていますね．\n",
        "\n",
        "それでは次は，先ほどのAEの出力層をSoftmaxにして，語彙数の分だけの面を持った歪なサイコロを再現するAuto Encoderを作成しましょう．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ugMm6izh8kRW",
        "outputId": "6643d865-7c2d-4a7d-ed9f-743bac9c8d07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=20, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=1000, bias=True)\n",
              "    (1): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 20]), torch.Size([32, 1000]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_features:int=1000, n_components:int=20):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_components = n_components\n",
        "        # build layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.n_components),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.n_components, self.in_features),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        encoded = self.encoder(x)\n",
        "        return encoded, self.decoder(encoded)\n",
        "\n",
        "ae = AutoEncoder()\n",
        "display(ae)\n",
        "x = torch.from_numpy(docs_tr[:32].toarray().astype(np.float32))\n",
        "encoded, decoded = ae(x)\n",
        "encoded.shape, decoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7PkReSirOSA"
      },
      "source": [
        "このAuto Encoderを訓練することで，それぞれの文書に対応した歪なサイコロを作成することができそうです．\n",
        "\n",
        "歪なサイコロはその文書を圧縮したベクトルを入力することで，decoderで作成することができるんですね．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dE2ind6F_8U"
      },
      "source": [
        "### 潜在ベクトルが正規分布から生成されていると仮定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K-gjI5RrewH"
      },
      "source": [
        "上で文書は多項分布から生成されると仮定しました．ではこれまで圧縮ベクトルや潜在ベクトルと呼んでいたものは，どんな分布から生成されると仮定できるでしょうか？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOT2yECzlurj"
      },
      "source": [
        "::::{note}\n",
        "ちょっと脱線：統計学的な文脈での __モデル__ と __モデリング__\n",
        "\n",
        "wip\n",
        "\n",
        "::::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYVvZO4a9Dq8"
      },
      "source": [
        "VAEではこの潜在ベクトルの各要素が正規分布からサンプルされていると仮定します．正規分布のパラメータは平均$\\mu$と標準偏差$\\sigma$なので，これをencoderに生成させましょう．\n",
        "潜在ベクトルの要素を$\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\sigma}）$で生成するようにAutoEncoderを書き換え， VariationalAutoEncoderクラスを作ってください．ただし， ミニバッチ学習させると仮定すると， muとsigmaはそれぞれがバッチサイズ$\\times$潜在次元数 の行列になります．\n",
        "\n",
        "::::{note}\n",
        "encoderの出力層は活性化関数なしのlinearにしましょう．これを推奨しているのは，tanhなど使ってmuやsigmaの取り得る範囲を限定しないためです．しかしながら，linearの出力をそのままsigmaとして使うと以下のようなエラーが発生します．\n",
        "```python\n",
        "batch_size = 8\n",
        "latent_dim = 5\n",
        "mu = torch.randn([batch_size, latent_dim])\n",
        "sigma = torch.randn([batch_size, latent_dim])\n",
        "z = torch.normal(mu, sigma)\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-17-1b89a1aa8b20> in <cell line: 5>()\n",
        "      3 mu = torch.randn([batch_size, latent_dim])\n",
        "      4 sigma = torch.randn([batch_size, latent_dim])\n",
        "----> 5 z = torch.normal(mu, sigma)\n",
        "\n",
        "RuntimeError: normal expects all elements of std >= 0.0\n",
        "```\n",
        "\n",
        "muはともかく，sigmaは標準偏差です．これが0未満になることはあり得ないので，encoderが直接sigmaを出力するように設計するのは無理があります．（もしかしたら上手い乱数でエラーにならない可能性もないわけではないですが...）\n",
        "\n",
        "そのため，encoderが直接に$\\sigma$を吐いていると仮定せずに， __対数分散__ を吐いていると仮定しましょう．\n",
        "\n",
        ":::{hint}  \n",
        "std（標準偏差）を求める行については，以下の等式を参考にしてください．\n",
        "\n",
        "$$\n",
        "e^{0.5 \\log (v a r)}=e^{\\log \\left(v a r^{0.5}\\right)}=var^{0.5}=\\sigma\n",
        "$$\n",
        "\n",
        "ref: [reparameterization trick in VAEs, How should we do this?](https://stats.stackexchange.com/questions/486158/reparameterization-trick-in-vaes-how-should-we-do-this)\n",
        "\n",
        "- 標準偏差: $\\sigma$  \n",
        "- 分散: $var = $\\sigma^2$  \n",
        "\n",
        "- これはつまり: $var^{0.5} = \\sigma$\n",
        "- expとlogは打ち消しあう\n",
        "\n",
        ":::  \n",
        "::::\n",
        "\n",
        "また，潜在ベクトルやencoder出力も見てみたいので，forwardメソッドの出力は，z, decoderの出力, mu, logvarにしましょう．\n",
        "\n",
        "[問題] AutoEncoderクラスを参考に，encoderが正規分布のパラメータを出力し，torch.normal関数で潜在変数ベクトルzを生成するようなVAEクラスを作成してください．\n",
        "\n",
        "::::{margin} Implementation tips\n",
        "実装方法としては以下の二つのどちらかになるでしょう．\n",
        "- encoderの最終層を二股にする\n",
        "- 2倍のユニット数を持つレイヤにしてから出力を2つに分割する\n",
        "\n",
        "::::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "rRHyO15r8tSh",
        "outputId": "a2ab79ae-0154-4953-f5c6-c175ae0d3e06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VariationalAutoEncoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=20, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=20, out_features=40, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=1000, bias=True)\n",
              "    (1): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 20]), torch.Size([32, 1000]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_features:int=1000, n_components:int=20):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_components = n_components\n",
        "        # build layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.n_components),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(self.n_components, self.n_components*2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.n_components, self.in_features),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        encoded = self.encoder(x)\n",
        "        mu = encoded[:, :self.n_components]\n",
        "        logvar = encoded[:, self.n_components:]\n",
        "        z = torch.normal(mu, torch.exp(logvar*0.5))\n",
        "        return z, self.decoder(z), mu, logvar\n",
        "\n",
        "ae = VariationalAutoEncoder()\n",
        "display(ae)\n",
        "x = torch.from_numpy(docs_tr[:32].toarray().astype(np.float32))\n",
        "z, decoded, mu, logvar = ae(x)\n",
        "z.shape, decoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94vq5qrnGTfO"
      },
      "source": [
        "### Reparameterization Trick: サンプリングが挟まっても微分可能な形へ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dyu-ZOX_Lgb"
      },
      "source": [
        "さて，正規分布をそのまま使うとbachpropagationでパラメータ更新ができるというNeural Netsの利点がなくなってしまいます．（サンプリングは微分できないよ）\n",
        "\n",
        "そのためこれを微分可能な操作で置き換える必要があります．\n",
        "\n",
        "視点を変えると，標準正規分布の平均をmuだけズラして，sigmaだけツリガネの幅を膨よかにしてあげれば任意の平均と標準偏差を持った正規分布が作れるはずです．\n",
        "この場合でも標準正規分布からのサンプリングが発生していますが，このサンプリングされた値を，エンコーダの入力値と同様に固定値だと考えれば，これ以外のmuとsigmaは微分可能な関数で作成されているので，ネットワーク全体も微分可能であり，backpropagation可能です．\n",
        "\n",
        "これを __reparameterization trick（再パラメータ化トリック）__ と呼びます．\n",
        "\n",
        "```python\n",
        "def reparameterization_trick(mu, logvar):\n",
        "    #z = Normal(mu,var)\n",
        "    std = (logvar*0.5).exp()\n",
        "    eps = torch.rand_like(mu)\n",
        "    z = eps * std + mu\n",
        "    return z\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWtKLAzDFFXF"
      },
      "source": [
        "[問題] これをVAEのメソッドとして実装し，サンプリングの代わりに使用するVAEクラスを作成してください．\n",
        "- ただし，訓練時にはreparameterization_trickを利用したサンプリングを行い，評価時にはmuをそのままzとして返すように工夫してください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "Z4cdTqyOaJyI",
        "outputId": "d918fb57-8fc9-48ea-8616-5d47180242a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VariationalAutoEncoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=20, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=20, out_features=40, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=1000, bias=True)\n",
              "    (1): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 40]), torch.Size([32, 1000]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_features:int=1000, n_components:int=20):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_components = n_components\n",
        "        # build layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.n_components),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(self.n_components, self.n_components*2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.n_components, self.in_features),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        encoded = self.encoder(x)\n",
        "        mu = encoded[:, :self.n_components]\n",
        "        logvar = encoded[:, self.n_components:]\n",
        "        z = self.reparameterization_trick(mu, logvar)\n",
        "        return encoded, self.decoder(z), mu, logvar\n",
        "\n",
        "    def reparameterization_trick(self, mu, logvar):\n",
        "        #z = Normal(mu,logvar)\n",
        "        #return z\n",
        "        if self.training:\n",
        "            std = (logvar*0.5).exp()\n",
        "            eps = torch.rand_like(mu)\n",
        "            z = eps * std + mu\n",
        "        else:\n",
        "            z = mu\n",
        "        return z\n",
        "\n",
        "\n",
        "ae = VariationalAutoEncoder()\n",
        "display(ae)\n",
        "x = torch.from_numpy(docs_tr[:32].toarray().astype(np.float32))\n",
        "latent_vector, p,*_ = ae(x)\n",
        "latent_vector.shape, p.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pml7e7A0wWbt"
      },
      "source": [
        "これで潜在ベクトルと文書ベクトルのそれぞれに対して確率分布を仮定することができました．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJcLC0gJU8BO"
      },
      "source": [
        "## 損失関数\n",
        "\n",
        ":::{note}  \n",
        "最終的な形：  \n",
        "ELBO = Cross Entropy + KL Divergence\n",
        "\n",
        "Cross Entropyはほとんど再構成誤差，KL Divergenceは正則化項の役割を行っているとも取れる．\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKRIFdHdOSXq"
      },
      "source": [
        "### Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTcdOATkHUYv"
      },
      "source": [
        "\n",
        "\n",
        "今回のサンプルデータはテキストデータであり，これのBoWを再構成するようなオートエンコーダをVAEで作るのが今回の目的でした．\n",
        "\n",
        "VAEの最終層の活性化関数を恒等関数（要はLinearの後に活性化関数を利用しない場合）にすれば，BoWを本当に再構成させることは可能かもしれません．\n",
        "\n",
        "しかしここまでで作成したVAEは，ネットワークの出力層（decoderの最終層）の活性化関数としてsoftmaxを利用しています．これは，ある文書がBoW vectorとして入力された時に全ての語彙に対して「その語彙がこの文書の中で出現する確率」を予測する問題にしたいからでした．\n",
        "\n",
        ":::{note}  \n",
        "単語の出現頻度はZipf則に従うことが知られています．\n",
        "\n",
        "> ジップの法則（ジップのほうそく、Zipf's law）あるいはジフの法則とは、出現頻度が k 番目に大きい要素が全体に占める割合がに比例するという経験則\n",
        "\n",
        ":::\n",
        "\n",
        "Softmaxを損失関数として利用しているので，損失関数は二乗和誤差のままではいけません．　このVAEは，言ってみればBoWで利用している語彙をクラスとして見ていて，文書をそれらに対応するようにクラス分類しているので，ここでは __Cross Entropy__ を利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M81_3gs3YukN"
      },
      "outputs": [],
      "source": [
        "# データの準備（仮のデータを生成）\n",
        "vocab_size = 1000\n",
        "n_docs = 1000\n",
        "docs_tr = np.random.rand(n_docs, vocab_size).astype(np.float32)\n",
        "docs_tr = torch.from_numpy(docs_tr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7XBbSVHUWI"
      },
      "source": [
        "[問題 h-1] BoWと，VAEの予測した単語出現確率を受け取ってCross Entropyを計算する関数を作成してください．\n",
        "\n",
        "::::{margin} Implementation tips\n",
        "\n",
        "cross entropyはデータごとの値が欲しいです．しかし実際にはバッチ学習を行いますので，この関数の返り値はバッチ平均を取ることになリます．  \n",
        "ex: batch_sizeが1でも32でもより大きな値でも，1つあたりのcross entropyと同じような大きさの値を出力させます．\n",
        "\n",
        "::::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LZAutW6d0er",
        "outputId": "434ad8bb-e837-4e6d-94c6-d15e41b4af86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(803.1685, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cross_entropy(pred_proba, bow):\n",
        "    return - (torch.log(pred_proba)*bow).sum(1).mean()\n",
        "cross_entropy(p, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLThmrv6OoeS"
      },
      "source": [
        "このCross EntropyはBoWの配列を参照し，element-wiseに負の対数尤度を単語数倍した値を返します．これを文書ごと（データごと）に合計し，これをその文書のcross entropyとしています．また，batch_size分だけ一気にデータが入力されるはずなので，batch_sizeで平均を取っています．\n",
        "\n",
        "Cross Entropyが小さくなるということは，元のコーパスにおいて出現頻度の大きかった単語が大きな出現確率を持っており，逆に出現頻度が小さかった単語が小さな出現確率になるということです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4EQ9qgFOTv1"
      },
      "source": [
        "### Kullback-Leibler divergence\n",
        "\n",
        "\n",
        "カルバック・ライブラー（リーブラ） ダイバージェンス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ-ntPxiOmfO"
      },
      "source": [
        "Cross Entropyだけを利用しても学習はできますが，VAEでは潜在変数が正規分布からサンプリングされていました．現状ではこれに対してなんの制約もつけていません．例えば元の入力データに対してノイズを付与し，それを取り除くようにオートエンコーダを学習させるものをDenoising Auto Encoderと呼びますが，現状はそれに近い形になっています．\n",
        "\n",
        "VAEでは潜在変数の事前分布を標準正規分布だと仮定しているのがキモになっているので，これを活かしながら何かしらの正則化を行い，この潜在変数が実際に取る値に制限をかけることを考えます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H1rr4jhHSaE"
      },
      "source": [
        "\n",
        "> 正則化は、学習時に用いる式に項を追加することによってとりうる重みの値の範囲を制限し、過度に重みが訓練データに対してのみ調整される（過学習する）ことを防ぐ役割を果たします。   \n",
        "> [正則化とは – 【AI・機械学習用語集】](https://zero2one.jp/ai-word/regularization/#:~:text=%E6%AD%A3%E5%89%87%E5%8C%96%E3%81%AF%E3%80%81%E5%AD%A6%E7%BF%92%E6%99%82%E3%81%AB,%E9%98%B2%E3%81%90%E5%BD%B9%E5%89%B2%E3%82%92%E6%9E%9C%E3%81%9F%E3%81%97%E3%81%BE%E3%81%99%E3%80%82&text=%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AB%E3%81%8A%E3%81%84%E3%81%A6%E9%81%8E%E5%AD%A6%E7%BF%92,%E9%81%B8%E6%8A%9E%E8%82%A2%E3%82%92%EF%BC%91%E3%81%A4%E9%81%B8%E3%81%B9%E3%80%82)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aoPCIeAQjWi"
      },
      "source": [
        "潜在ベクトルの真の母集団は標準正規分布に従っていると仮定しましたが，実際に私たちが集めてきたデータはそのサブセットにすぎません．ということはデータに偏りがあるはずなので，綺麗な標準正規分布に従っているとは思えません．そのため実際には，潜在変数（潜在ベクトル$\\mathbf{z}$のそれぞれの要素）が独立したガウス分布に従っているだろうと我々は仮定したのでした．母集団が標準正規分布に従っているのですから，そのサブセットである訓練データの潜在変数もここから大きく離れたガウス分布に従っているとは思えませんので，平均と標準偏差が大きくなりすぎて必要以上にガウス分布が大きくなりすぎるのを抑制したくなります．\n",
        "\n",
        "そこで標準正規分布とencoderの構築した正規分布とが似たような形になるように正則化を行います．つまり標準正規分布から離れれば離れるほどに罰則を与える（値が大きくなる）ような関数を導入します．これに二つの確率分布の疑似距離を求める __Kullback-Leibler divergence　(KLダイバージェンス)__ を使います．KLダイバージェンスは確率分布Pと確率分布Qとがどれだけ似ているのかを測ります．\n",
        "\n",
        "$$\n",
        "D_{K L}(P \\| Q)=\\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} d x\n",
        "$$\n",
        "\n",
        "ここでPとQが全く同じであれば0を取り，違う形であればあるほど値は大きくなります．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXC1HqORk3Ux"
      },
      "source": [
        "また，Qが標準正規分布であるならば，式を整理して単純にできます．\n",
        "\n",
        "encoderが作成したガウス分布と標準正規分布とのKLダイバージェンスを求める式：\n",
        "\n",
        "$$\n",
        "D_{KL}[N(\\mu,\\sigma)||N(0,1)] = -\\frac{1}{2}\\sum(1+\\log(\\sigma^2)-\\mu^2-\\sigma^2)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipxpyJ28n3w5"
      },
      "source": [
        "[問題 h-2] 上記のKLダイバージェンスの式をPythonで実装してください．ただし，文書（データ）ごとにこの値を求めた上で，バッチ平均を出力します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lLS5VbTrk5qM"
      },
      "outputs": [],
      "source": [
        "def kld(mu,sigma):\n",
        "    kl = -0.5 * torch.sum(1 + torch.log(sigma**2) - mu**2 - sigma**2)\n",
        "    return kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW-gJ0-tW8rC",
        "outputId": "5660dd67-f86e-4a25-d7ae-c02ae07f26db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KL Divergence: 985.9890747070312\n"
          ]
        }
      ],
      "source": [
        "_, _, mu, logvar = ae(x)\n",
        "kl_loss = kld(mu, logvar)\n",
        "print(f\"KL Divergence: {kl_loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5qUUOPYBOEY"
      },
      "source": [
        "encoderの作るガウス分布は標準正規分布に近くなるように矯正していくわけですが，全く標準正規分布と同じようになってしまっても困ります．これはつまり`mu=torch.zeros([m,k]), sigma=torch(torch.ones([m,k]))`なので，encoderの出力がどんなデータが入ってきても同じになってしまいます．つまりencoderの影響が全くない状態になるわけです．ここまでやってしまうと困ってしまうので，実際にはKLダイバージェンスが0に近いけど完全に0ではない状態で学習が止まってくれればちょうどいいわけです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0eSqpkBoJH9"
      },
      "source": [
        "###  Evidence Lower BOund (ELBO, 変分下限)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71h0AfGSoV0C"
      },
      "source": [
        "文書ごとの対数尤度から，先ほど説明したKLダイバージェンスを引いた値を __ELBO__ と呼びます．VAEではこれを最大化するようにパラメータを更新します．ただし，ニューラルネットワークの慣例に倣って最小化問題にしたいので，それぞれの項にマイナスをかけたNegative ELBOを実際の目的関数として利用します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8zOIk1umhTI"
      },
      "source": [
        "目的関数\n",
        "\n",
        "$$\n",
        "\\text{Negative ELBO} = \\operatorname{Cross Entropy}(p_{vae}(\\hat{x} | \\hat{z}), p(x)) + \\operatorname{KL Divergence}(\\mathcal{N}(0,1) || \\mathcal{N}_{vae}(\\hat{\\mu}, \\hat{\\sigma}))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFm6wWMFMES"
      },
      "source": [
        "## メトリクス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AepslQoNFdIx"
      },
      "source": [
        "学習中や学習後のモデルの評価を行う関数をメトリクスと呼びます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btxpx1rNFPGP"
      },
      "source": [
        "### Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfK2jMRkFSLG"
      },
      "source": [
        "__Perplexity__ は言語モデルの性能（予測精度）を評価するための指標の一つです．一般的な言語モデルだと「This is a __ 」のように`This is a`までを生成したら，その次の単語を予測するような処理を行っています．この時， __ に入る単語の候補の数は，言語モデルが賢くなればなるほど数が減っていくはずです． この次の単語の選択肢の数の平均を Perplexity と呼びます．\n",
        "\n",
        "Perplexityは\n",
        "\n",
        "$$ PPL = 2^{(\\text{1単語あたりのエントロピー})} $$\n",
        "\n",
        "で求められます．また通常， Perplexity はテストセットに対して求めます．\n",
        "\n",
        "今回，「1単語あたりのエントロピー」に相当するのは，一単語あたりのクロスエントロピーです． 現在は一つの文章あたりの値を出しているので，少し変えてバッチ内の単語数で平均をとるようにすればOKです．\n",
        "\n",
        "また，上の定義では2の冪乗になっていますが，計算する際にはexpを利用しましょう．\n",
        "\n",
        ":::{hint}\n",
        "\n",
        "2の冪乗でPerplexityを計算する場合は，この値の最大値は語彙数になるはずです．\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONhMaP3IH3z4"
      },
      "source": [
        "[問題 h-3] Perplexityを求める関数を作成してください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ApyrHpYuH9lV"
      },
      "outputs": [],
      "source": [
        "def perplexity(pred_proba, bow):\n",
        "    ppl = torch.exp(cross_entropy(pred_proba, bow)*bow.shape[0]/bow.sum())\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlLWqDmFXElp",
        "outputId": "4ed099e2-f309-4f8b-a6f7-8bf2c8423eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 1108.4801025390625\n"
          ]
        }
      ],
      "source": [
        "ppl = perplexity(p, x)\n",
        "print(f\"Perplexity: {ppl.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTS9SmIuEruw"
      },
      "source": [
        "### 訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCCkUs6nEw59"
      },
      "source": [
        "ここまでで作成したVAEはNVDM (Neural Variational Document Model){cite}`Miao2016-xh`と呼ばれています．\n",
        "\n",
        "[問題 h-4] 自作の訓練スクリプトやskorchを利用して，VAE（NVDM）を訓練してください．\n",
        "\n",
        "hyper-paramsの設定例:\n",
        "- epoch: 100\n",
        "- batch_size: 128\n",
        "- learning_rate: 0.01\n",
        "- optimizer: Adam\n",
        "- latent_dim: 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OBM4nU-iaGKL"
      },
      "outputs": [],
      "source": [
        "# データの準備\n",
        "train_data, test_data = train_test_split(docs_tr, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(TensorDataset(train_data), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(test_data), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sR4mG8fcS9k9"
      },
      "outputs": [],
      "source": [
        "def train_vae(model, train_loader, test_loader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            _, recon, mu, logvar = model(batch[0])\n",
        "            ce_loss = cross_entropy(recon, batch[0])\n",
        "            kl_loss = kld(mu, logvar)\n",
        "            loss = ce_loss + kl_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          train_ppl = perplexity(model(train_data)[1], train_data)\n",
        "          test_ppl = perplexity(model(test_data)[1], test_data)\n",
        "        print(f\"Training Perplexity: {train_ppl:.4f}\")\n",
        "        print(f\"Test Perplexity: {test_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC_6F03bEw3s"
      },
      "source": [
        "[問題 h-5] 訓練終了時のtraining data全体とtest data全体に対するPerplexityをそれぞれ求めてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6B2N7IiZc5_"
      },
      "source": [
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_ppl = perplexity(model(train_data)[1], train_data)\n",
        "    test_ppl = perplexity(model(test_data)[1], test_data)\n",
        "\n",
        "print(f\"Training Perplexity: {train_ppl:.4f}\")\n",
        "print(f\"Test Perplexity: {test_ppl:.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cABsMGLyU8BO"
      },
      "source": [
        "### 類似単語検索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeX621j3Yzrk"
      },
      "source": [
        "decoderのweightは潜在次元数$\\times$語彙数 の行列になっています． shopeが逆ならば転置してください．これを（正規されていませんが，）トピックごとの単語分布だと見ることが可能です．word2vecの実装で登場した類似単語検索の関数にこのVAEが獲得したトピック-単語行列を適用し，適当な単語をクエリとして実験してみてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj_XaAMG4cpz",
        "outputId": "2d807b9f-75ba-472b-dac0-89c4a8cee61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topic_word_distribution.shape=torch.Size([20, 1000])\n"
          ]
        }
      ],
      "source": [
        "topic_word_distribution = ae.decoder[0].weight.T\n",
        "print(f\"{topic_word_distribution.shape=}\")\n",
        "\n",
        "topic_word_distribution = topic_word_distribution.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtB4bcInU8BO"
      },
      "source": [
        "### 可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxjogfUTKbRq"
      },
      "source": [
        "トピックの単語分布の中で，特に大きい値を持っている単語は「そのトピックでよく出現した単語」であると考えられます．これを上位n個ずつ表示することで，そのトピックが何について話されたものなのかを想像することができそうです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dlFghxpIJEX6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_topics(topic_word_distribution, id2word:list[str], topn:int=5):\n",
        "    for topic_number, each_word_distribution in enumerate(topic_word_distribution):\n",
        "        sorted_index = np.argsort(each_word_distribution)[::-1]\n",
        "        print_message = \" \".join([id2word[index] for index in sorted_index[:topn]])\n",
        "        print(f\"topic {topic_number:02}:\\t\", print_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoIVAltNLxro",
        "outputId": "833219f7-302e-4843-87bd-ece756411ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topic 00:\t thinking 25 ideas section start\n",
            "topic 01:\t package cards 15 60 images\n",
            "topic 02:\t distribution voice extra jews according\n",
            "topic 03:\t response school worth getting heard\n",
            "topic 04:\t send trade received basic chips\n",
            "topic 05:\t community good market nature women\n",
            "topic 06:\t available basically north currently algorithm\n",
            "topic 07:\t took ax context break death\n",
            "topic 08:\t products area gives hot number\n",
            "topic 09:\t court supposed explain gun later\n",
            "topic 10:\t engine consider advance legal model\n",
            "topic 11:\t values day solution need carry\n",
            "topic 12:\t statement digital true david groups\n",
            "topic 13:\t open possibly end involved cs\n",
            "topic 14:\t heard keyboard necessary saw book\n",
            "topic 15:\t division considered said gov board\n",
            "topic 16:\t conference unfortunately ve love logic\n",
            "topic 17:\t air doing company changes anybody\n",
            "topic 18:\t hope ax programs evidence peace\n",
            "topic 19:\t stop wrong base cx god\n"
          ]
        }
      ],
      "source": [
        "id2word = list(word2id.keys())\n",
        "print_topics(topic_word_distribution, id2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM1vdeea6IV-"
      },
      "source": [
        "実際には，あまり意味のない単語列が表示されているかも知れません．これは以下の理由が考えられます．\n",
        "- 数字やstopword，単語の原形化や見出し語化など前処理を適切に行なっていない\n",
        "- 出現頻度は大きいが意味のない単語が多すぎる\n",
        "\n",
        "これから意味のある単語列を表示するにはterm-scoreという指標で並び替えることを考えますが，これは調べてみてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie5Z5v9eU8BO"
      },
      "source": [
        "## Gaussian Softmax Trickを使ったVAE（GSM）による文書モデリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQRBDpW-U8BO"
      },
      "source": [
        "Gaussian Softmax Model{cite}`GSM`\n",
        "\n",
        "GSMの気持ち：  \n",
        "潜在ベクトルの仮定する事前分布は，「本当に標準正規分布でいいの？」→もっと良い確率分布がありそう→Dirichlet分布を使いたい→VAEでは使い辛い→ガウス分布を使って近似しよう！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-0i3iIKU8BO"
      },
      "source": [
        "---\n",
        "w.i.p.\n",
        "\n",
        "これまでのVAEとの違い：\n",
        "- zはGaussian Sampleそのままだったが，これに出力層がSoftmaxになっているMLPを適用する．\n",
        "\n",
        "同じ空間上の点として「単語・文書・トピックの中心」を可視化するために，これらを埋め込む空間を用意します．\n",
        "- decoderのweightを$k \\times L$のトピック重心と，$v \\times L$の単語埋め込みとの内積であるとする．\n",
        "    - ここで$L$は単語埋め込みベクトルの次元数\n",
        "    - 単語は単語埋め込みベクトル，トピックはトピック重心ベクトルをそれぞれ利用して二次元に圧縮した後に散布図にします．\n",
        "    - 文書は$k$次元に圧縮されていますが，潜在表現ベクトルとトピック重心行列との内積を取ることで文書埋め込みベクトルが作成できます．あとは他のものと同様に二次元に圧縮して散布図にします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWlSX54DfhB"
      },
      "source": [
        "[問題 h-6] $k$次元の潜在表現を入力として受け取り，$L$次元にするようなニューラルネットを作成してください（このネットワークをGeneratorと呼びましょう．）．ただし，簡単のために一層のNNであり，活性化関数はSoftmaxです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "095SyEHYE6n7"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.fc(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuuidItnD1_z"
      },
      "source": [
        "[問題 h-7] pytorchの[nn.Linearクラス](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)を修正し，単語埋め込み行列とトピック重心行列をパラメータとして持たせ，全結合層と同様の処理を行えるようにしてください．これをFactorizedLinearクラスとしてください．\n",
        "\n",
        "- self.weightを削除\n",
        "- nn.Parameterクラスを使い，新しいパラメータとしてword_embeddingsとtopic_centroidsを用意．\n",
        "    - 初期化はnn.Linearのweightを参考にしてもいいですし，nn.Embeddingを参考にしても構いません．\n",
        "- @propertyデコレータを使ってweightを返せるか検討して下さい\n",
        "    - これが難しい場合はget_weightメソッドを定義して，weightを返してください．\n",
        "- forwardメソッドで全結合層と同様の結果をもたらす処理を書いてください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SF3XY4uPGAWc"
      },
      "outputs": [],
      "source": [
        "class FactorizedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.word_embeddings = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.topic_centroids = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.word_embeddings, a=np.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.topic_centroids, a=np.sqrt(5))\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return torch.matmul(self.word_embeddings, self.topic_centroids)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.matmul(input, self.weight.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJ3r59dF5K1"
      },
      "source": [
        "[問題 h-8] GeneratorとFactorizedLinearをVAEクラスに導入し，GaussianSoftmaxクラスを作成してください．また，VAEと同様に訓練をおこなって，test setに対するPerplexityを表示してください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HPNNZeGuT3M2"
      },
      "outputs": [],
      "source": [
        "class GaussianSoftmax(nn.Module):\n",
        "    def __init__(self, in_features, n_components, n_topics):\n",
        "        super().__init__()\n",
        "        self.vae = VariationalAutoEncoder(in_features, n_components)\n",
        "        self.generator = Generator(n_components, n_topics)\n",
        "        self.factorized_linear = FactorizedLinear(n_topics, in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded, _, mu, logvar = self.vae(x)\n",
        "        z = self.vae.reparameterization_trick(mu, logvar)\n",
        "        topic_dist = self.generator(z)\n",
        "        reconstructed = self.factorized_linear(topic_dist)\n",
        "        return encoded, reconstructed, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHGiCAcrYV6v",
        "outputId": "c4383011-13f8-4d72-84e4-74d1a51c6f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: nan\n",
            "Training Perplexity: nan\n",
            "Test Perplexity: nan\n",
            "Epoch 2/5, Loss: nan\n",
            "Training Perplexity: nan\n",
            "Test Perplexity: nan\n",
            "Epoch 3/5, Loss: nan\n",
            "Training Perplexity: nan\n",
            "Test Perplexity: nan\n",
            "Epoch 4/5, Loss: nan\n",
            "Training Perplexity: nan\n",
            "Test Perplexity: nan\n",
            "Epoch 5/5, Loss: nan\n",
            "Training Perplexity: nan\n",
            "Test Perplexity: nan\n",
            "GaussianSoftmax Test Perplexity: nan\n"
          ]
        }
      ],
      "source": [
        "# GaussianSoftmaxモデルの訓練\n",
        "gs_model = GaussianSoftmax(vocab_size, 20, 20)\n",
        "gs_optimizer = torch.optim.Adam(gs_model.parameters())\n",
        "\n",
        "# 訓練の実行\n",
        "train_vae(gs_model, train_loader, test_loader, gs_optimizer, num_epochs=5)  # エポック数を適宜調整してください\n",
        "\n",
        "# テストデータに対するPerplexityの計算\n",
        "gs_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_ppl = perplexity(gs_model(test_data)[1], test_data)\n",
        "\n",
        "print(f\"GaussianSoftmax Test Perplexity: {test_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baF5N01-U8BO"
      },
      "source": [
        "### 類似単語検索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTJE3WonF6jX"
      },
      "source": [
        "[問題 h-9] VAEの例を参考に類似単語検索を行って下さい．ただしこれまではdecoderのweightを利用していましたが，ここではFactorizedLinearインスタンスのword_embeddingsを使ってください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_hbYfC1MT4z2"
      },
      "outputs": [],
      "source": [
        "def find_similar_words(word_embeddings, word2id, id2word, target_word, top_n=10):\n",
        "    target_index = word2id[target_word]\n",
        "    target_vector = word_embeddings[target_index]\n",
        "\n",
        "    similarities = torch.cosine_similarity(target_vector.unsqueeze(0), word_embeddings)\n",
        "    top_indices = torch.argsort(similarities, descending=True)[1:top_n+1] \n",
        "\n",
        "    similar_words = [(id2word[idx.item()], similarities[idx].item()) for idx in top_indices]\n",
        "    return similar_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fMhdVBtU8BO"
      },
      "source": [
        "### 可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIdeO3QQU8BP"
      },
      "source": [
        "[問題 h-10] 以下の手順で埋め込み空間の散布図を作成してください．\n",
        "\n",
        "1. テストデータ全てに対する文書埋め込みベクトルを作成してください．\n",
        "2. 以下の三種類のベクトルを，TSNEを用いて2次元に圧縮した後，散布図として表示してください．\n",
        "    - 単語埋め込みベクトル\n",
        "    - トピック重心ベクトル\n",
        "    - 文書埋め込みベクトル\n",
        "        - 全ての文書の埋め込みベクトルを使うと多すぎるので，以下の方法で代表的な文書を選択します：\n",
        "            1. それぞれのトピックのトピック重心ベクトルと文書埋め込みベクトルとのコサイン類似度を計算\n",
        "            2. 類似度が高い順に10個の文書をピックアップ（つまり20トピックならばそれぞれ10文書で200文書だけを利用します）\n",
        "\n",
        "ただし，3種類のベクトルはそれぞれ別の色で表示し，可能であればそれぞれの点にラベルをつけてください．例えば：\n",
        "    - \"pen\"という単語ならばこれが緑色のラベルとして点の近く or 点の代わりとして表示します．\n",
        "    - トピック番号0~19については\"【Topic 0】\"のような赤色のラベルを使います．\n",
        "    - 文書であれば先頭のいくつかの単語だけを青色でラベルとして利用します．\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FirBx7hLJobt"
      },
      "outputs": [],
      "source": [
        "def visualize_embedding_space(word_embeddings, topic_centroids, doc_embeddings, id2word, selected_docs):\n",
        "    all_embeddings = torch.cat([word_embeddings, topic_centroids, doc_embeddings], dim=0)\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    embeddings_2d = tsne.fit_transform(all_embeddings.detach().cpu().numpy())\n",
        "\n",
        "    n_words = word_embeddings.shape[0]\n",
        "    n_topics = topic_centroids.shape[0]\n",
        "    words_2d = embeddings_2d[:n_words]\n",
        "    topics_2d = embeddings_2d[n_words:n_words+n_topics]\n",
        "    docs_2d = embeddings_2d[n_words+n_topics:]\n",
        "\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.scatter(words_2d[:, 0], words_2d[:, 1], c='green', alpha=0.5, label='Words')\n",
        "    plt.scatter(topics_2d[:, 0], topics_2d[:, 1], c='red', alpha=0.5, label='Topics')\n",
        "    plt.scatter(docs_2d[:, 0], docs_2d[:, 1], c='blue', alpha=0.5, label='Documents')\n",
        "\n",
        "    for i in range(min(20, n_words)):\n",
        "        plt.annotate(id2word[i], (words_2d[i, 0], words_2d[i, 1]), color='green', alpha=0.7)\n",
        "\n",
        "    for i in range(n_topics):\n",
        "        plt.annotate(f\"Topic {i}\", (topics_2d[i, 0], topics_2d[i, 1]), color='red', alpha=0.7)\n",
        "\n",
        "    for i in range(min(20, len(selected_docs))):\n",
        "        plt.annotate(f\"Doc {i}\", (docs_2d[i, 0], docs_2d[i, 1]), color='blue', alpha=0.7)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(\"Embedding Space Visualization\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wxfGgxdQYklY"
      },
      "outputs": [],
      "source": [
        "# 使用例\n",
        "gs_model.eval()\n",
        "with torch.no_grad():\n",
        "    doc_embeddings, _, _, _ = gs_model(test_data)\n",
        "\n",
        "# 代表的な文書の選択（簡単のため、最初の200文書を使用）\n",
        "selected_docs = [f\"Doc {i}\" for i in range(200)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHWAwvric1N7"
      },
      "source": [
        "## 参考文献\n",
        "\n",
        "\n",
        "```{bibliography}\n",
        ":filter: docname in docnames\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIvV8_Rlc1N8"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
